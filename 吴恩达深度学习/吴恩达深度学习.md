# 吴恩达深度学习

- [笔记来源](https://www.bilibili.com/video/BV1FT4y1E74V?from=search&seid=11455456919575072232&spm_id_from=333.337.0.0)

- 笔记时间:2022.03.09

## 1 介绍

### 1.1 神经网络简介

- 应用范围很多:  实时估计  在线广告  图像处理 (CNN)  文本翻译(RNN)   语音识别(RNN)   自动驾驶 等等

- ![image-20220309095503797](images/image-20220309095503797.png)

- 结构化数据:表格,每个数据都有定义

  非结构化:图像,音频视频,文字

### 1.2 深度学习崛起

- 规模变化,不论是数据还是神经网络![image-20220309100003356](images/image-20220309100003356.png)



## 2 神经网络编程基础

### 2.1 二分类

- 约定符号,需要注意特征向量使用列向量

  ![image-20220309101236748](images/image-20220309101236748.png)



### 2.2 逻辑回归问题

- 预测样本属于某个类型的概率

  不会使用右上的矩阵(偏置和权重合并为一个矩阵)

  ![image-20220309101646588](images/image-20220309101646588.png)

- 损失函数
  - 不适用MSE,会变成非凸函数,  使用交叉熵衡量概率分布
  - ![image-20220309102128660](images/image-20220309102128660.png)

### 2.3 梯度下降法

- 最小化损失函数 → 凸函数 → 求导 → 梯度下降 → 迭代

- 计算流程图:将计算的步骤按先后关系用流程图表示

  ![image-20220309103103406](images/image-20220309103103406.png)

- 求导:链式法则,代码中可以直接使用下标表示导数 比如 dv代表dJ/dv

  ![image-20220309103649091](images/image-20220309103649091.png)

- 逻辑回归的导数

  需要注意的是dσ是对sigmoid求导

  ![image-20220309104920406](images/image-20220309104920406.png)

- 扩展到m个样本的梯度下降使用向量进行加速

  ![image-20220309110959464](images/image-20220309110959464.png)



- python中的代码实现

  ![image-20220309111348397](images/image-20220309111348397.png) 

- 使用numpy的矩阵时候,尽量指定好维度

### 2.4 逻辑回归损失函数选择

- ![image-20220309134732495](images/image-20220309134732495.png)



![image-20220309134947522](images/image-20220309134947522.png)



## 3 单隐藏层网络

### 3.1 神经网络概览

- ![image-20220309140620075](images/image-20220309140620075.png)

### 3.2计算过程,向量化

![image-20220309141100612](images/image-20220309141100612.png)

![image-20220309142659342](images/image-20220309142659342.png)

![image-20220309143414586](images/image-20220309143414586.png)

### 3.3 激活函数

- sigmoid 最后一层(输出层)更好

- tanh 工作情况比sigmoid好,大多数情况下

- ReLU 可以默认使用,问题在于原点处没有导数

  可以使用 leaky ReLU,速度快

- ![image-20220309150355995](images/image-20220309150355995.png)

- 为什么需要非线性激活函数 -- 线性组合也是线性关系,隐藏层便没作用了

### 3.4 激活函数导数

- sigmoid → g' = g (1-g)
- tanh → g'=1- g^2^
- ReLU → g' = 1,x≥0;g'=0,x<0
- Leaky ReLU → g'=1,x≥0;g'=0.01,x<0

### 3.5 梯度下降法

- ![image-20220309155345630](images/image-20220309155345630.png)

### 3.6 随机初始化

- 假如数据相同,权重就会相同,导致重复冗余

- w初始化为随机值再比较小的范围内,因为激活函数斜率较大;

  可以乘以一个常数,比如 0.01

## 4 深度神经网络

### 4.1深层神经网络

- 具有多层隐藏层的神经网络

  ![image-20220309155917831](images/image-20220309155917831.png)

- 符号

  ![image-20220309160134344](images/image-20220309160134344.png)

### 4.2 前向和反向传播

- ![image-20220309161121329](images/image-20220309161121329.png)
- ![image-20220309160400767](images/image-20220309160400767.png)``

### 4.3 核对矩阵维数

- ![image-20220309161723816](images/image-20220309161723816.png)

### 4.4 为什么深度网络效果更好

- ![image-20220309162404812](images/image-20220309162404812.png)



- 单层网络计算异或规模需要很大

### 4.5 搭建神经网络

- ![image-20220309163014127](images/image-20220309163014127.png)

- 超参数 -- 学习率 迭代数 隐藏层个数 几乎偶函数 正则项等等 ....  实际上用于控制参数 W和 b

  超参数设置基于经验,也就是尝试

## 5 创建机器学习应用 

### 5.1 数据集

- 划分为三部分 训练集 交叉验证集 测试集

  数据量小 7:3 或者 8:2

  假如百万数据,可能98:1:1,几千条当验证/测试集就够了

- 数据集来源需要来自同一分布,不能说一个来自手机,一个来自网络也抓图

- 方差与偏差   ===>  过拟合,欠拟合

  ![image-20220310093846467](images/image-20220310093846467.png)

### 5.2 正则化

- L2正则:  λ变大,w变小,z变小,在σ里面会相当于线性函数]

  ![image-20220310094825593](images/image-20220310094825593.png)

- Dropout正则化:   节点概率失活

  - 反向随即失活 -- 训练阶段,失活导致期望变小,需要放大

  - 测试阶段禁止失活,没必要放大

  - Dropout功能类似L2正则化

  - 应用: 计算机视觉

    缺点:  损失函数一直在变化,无法调试

- 数据扩增:  反转   裁切  扭曲

- 提前停止:   识别误差够小就直接停止

- 归一化输入:

  ![image-20220310104149666](images/image-20220310104149666.png)

### 5.3 梯度消失/爆炸

- 因为层次之间是乘法关系,W变化一些就会指数增长

- 单神经元初始化

  ![image-20220310105429074](images/image-20220310105429074.png)

- 梯度数值估计: 利用临近的左右两点计算梯度,假如相差在允许范围内,则梯度计算正确

  只能检测用,不可以和dropout一起使用

 ![image-20220310105949823](images/image-20220310105949823.png)



## 6 算法优化

### 6.1 小批量梯度下降法

- 把大数据集划分为若干个小数据集,计算梯度的时候从 mini-batch 进行计算

- 当mini-batch集合大小为 1,就是随机梯度下降;

  当大小为全部数据,就是批量下降

  ![image-20220310112455944](images/image-20220310112455944.png)

![image-20220310112644518](images/image-20220310112644518.png)

- 损失函数下降图

  ![image-20220310112857042](images/image-20220310112857042.png)

### 6.2 动量梯度下降法

- 移动平均数:  用前N天的平均数预测预期  

- 指数平滑法是对加权移动平均法的改进，它是将前期预测值和前期实际值分别确定不同的权数

- V~t~是预测值,θ~t~是实际值

  ![image-20220310114509504](images/image-20220310114509504.png)

- 指数加权平均数

  - ![image-20220310113637172](images/image-20220310113637172.png)

- ![image-20220310115138918](images/image-20220310115138918.png)

- ![image-20220310115653757](images/image-20220310115653757.png)

- momentum算法

  - ![image-20220310135637518](images/image-20220310135637518.png)

    ![image-20220310135851094](images/image-20220310135851094.png)

### 6.3 RMSprop

- ![image-20220310141200668](images/image-20220310141200668.png)

### 6.4 Adam

- ![image-20220310141705676](images/image-20220310141705676.png)

### 6.5 学习率衰退

- ![image-20220310143103766](images/image-20220310143103766.png)

  ![image-20220310143149757](images/image-20220310143149757.png)



### 6.6 局部最优

- 当参数较多的时候不太可能出现
- 优化器(Adam,momentum..)可以走出鞍点

## 7 超参数调试

### 7.1 调试处理

- 超参数重要程度  红 橙 紫

  ![image-20220310144003615](images/image-20220310144003615.png)

- 当两个超参的时候:  p1-p2图  使用随机值,不要使用网格

  由粗到细搜索合适参数

- 范围确定

  - 学习率α范围[10^-4^,10^0^], β使用范围 [0.9,0.999],使用log尺度
  - 其他参数可以使用随机值

- 超参选择
  - 并行训练多个模型
  - 一个模型逐个参数优化测试

### 7.2 归一化

- 对输入进行归一化,称为 Batch Norm

  隐藏层的输入进行均值归一化,

  ![image-20220310150022239](images/image-20220310150022239.png)

- ![image-20220310150826358](images/image-20220310150826358.png)

- BN作用:
  - 缩放输入数据到相似范围
  - 经过感知机处理的数据分布跟原来可能不一致,重新归一化保证均值和方差不变

- 测试时使用训练集留下的 μ和σ
  
  - ![image-20220310152316472](images/image-20220310152316472.png)

### 7.3 Softmax回归

- 多分类激活函数

  ![image-20220310152854636](images/image-20220310152854636.png)

- 损失函数

  ![image-20220310153441131](images/image-20220310153441131.png)



## 8 机器学习策略

### 8.1 正交化

- 单一数字评价指标,
  - 比如混淆矩阵可以使用F1评价精度和召回率
  - 均值/加权平均
  - 阈值下最优值
- 数据集划分
  - 最好洗牌再划分
  - 训练集和测试集要来自同一分布
  - 训练:验证:测试 = 6:2:2,大数据集测试集几千就够用了

- 改变评价指标
  - 正确率 和 错误率![image-20220310170353514](images/image-20220310170353514.png)

### 8.2 为什么是人的表现

- 贝叶斯最佳错误率

  超过人类之后增速变慢,因为人类可以手动校验,处理偏差 

  ![image-20220310170757075](images/image-20220310170757075.png)

- 可避免偏差![image-20220310171508556](images/image-20220310171508556.png)

- 超越人类 -- 结构化数据识别,自然感知方面达到人类级别较难

## 9 误差分析

### 9.1 分析误差

- 查看错误集合里面多少是被错误分类的,看改善空间有多少

- 清除错误标签 -- 错误足够随机,数据集够大,不会影响

  ​						-- 系统性错误就会有问题
  
  ​						-- 验证/测试集的数据可以进行矫正
  
- 搭建模型,快速迭代 -- 模型不用很复杂,实现基本功能即可

### 9.2 数据集

- 当数据集分布差异较大,可以考虑分层抽样
- 使用交叉验证
- 训练集和测试集分布差异较大,需要重新划分,人工查看差异

### 9.3 迁移学习

- 在已经训练好的模型,训练新的输出; 模型称为预训练模型

  因为图像识别低层大部分是一致的,线条 边缘等

- 多任务学习

  任务类似,可以共用低层

  ![image-20220310181447666](images/image-20220310181447666.png)

### 9.4 端到端学习

- 用一个大的神经网络直接替代流水线

  ![image-20220310182310871](images/image-20220310182310871.png)

![image-20220310182756134](images/image-20220310182756134.png)



- 使用情况:
  - 数据量足够大,比如语音识别,脸部识别
  - 会排除掉中间过程

## 10 卷积神经网络

### 10.1 计算机视觉

- 假设一张1000*1000的图片直接使用深度学习,那么输入参数就有一百万,对应的权重系数将会数以亿计
- 使用卷积神经网络进行运算处理
- 边缘检测
  - 假设要检测垂直方向,那么就意味着水平方向变化剧烈的是边缘,所以核使用水平方向的求导
  - ![image-20220311091624607](images/image-20220311091624607.png)
  - ![image-20220311091842216](images/image-20220311091842216.png)
  - 其他类型检测
    - 从上图的卷积结果可以判断边缘左边比右边亮
    - ![image-20220311092356932](images/image-20220311092356932.png)



### 10.2 一些操作

- Padding(填充):

  - 从上图可以看出卷积之后图像大小会变小,假如不变的话需要对原始图像进行边缘填充

  - 填充内容可以全是0,图片镜像等等

  - 卷积核一般是奇数

    ![image-20220311093000799](images/image-20220311093000799.png)

- Strided (步长): 卷积窗口移动距离

  ![image-20220311093225189](images/image-20220311093225189.png)

- 数学中卷积核需要先镜像翻转,为了可以有结合律;而深度学习不用

- 三维卷积
  - ![image-20220311093831341](images/image-20220311093831341.png)
  - ![image-20220311094039302](images/image-20220311094039302.png)

### 10.3 卷积网络

- 单层卷积网络

  ![image-20220311095626881](images/image-20220311095626881.png)

- 符号标记

  ![image-20220311100335679](images/image-20220311100335679.png)

- 池化层
  - max pooling,  average pooling
  - 很少用padding,提取低层特征
  - ![image-20220311101041581](images/image-20220311101041581.png)

- 全连接层 : 之前的神经网络结构

- 使用卷积: 参数共享 稀疏连接

## 11 案例分析

### 11.1 经典网络

- 网络结构越来越深,单元化越来越明显,结构变得统一

- ![image-20220311103359818](images/image-20220311103359818.png)

- ![image-20220311103704276](images/image-20220311103704276.png)
- ![image-20220311104015540](images/image-20220311104015540.png)

### 11.2 残差网络

- 

- ![image-20220311104450717](images/image-20220311104450717.png)
- ![image-20220311105016761](images/image-20220311105016761.png)



- 1x1卷积:![image-20220311105511309](images/image-20220311105511309.png)
- ![image-20220311105630311](images/image-20220311105630311.png)

### 11.3 Inception

- ![image-20220311105904229](images/image-20220311105904229.png)

- ![image-20220311110235390](images/image-20220311110235390.png)

- ![image-20220311110631306](images/image-20220311110631306.png)



### 11.4 建议

- 迁移训练
- 数据扩增
- 选择框架

## 12 物体检测

### 12.1 物体定位

- 图片分类 + 定位,最后输出的时候输出额外参数

  ![image-20220311112315843](images/image-20220311112315843.png)

- 特征点检测

  ![image-20220311112727317](images/image-20220311112727317.png)

### 12.2 目标检测

- 滑动窗口![image-20220311112905576](images/image-20220311112905576.png)

- 卷积替代全连接实现滑动窗口

  ![image-20220311113325727](images/image-20220311113325727.png)

- 边框预测 -- 由于步长的关系可能导致边框不准确
  - YOLO![image-20220311113924008](images/image-20220311113924008.png)



- 交叉比(Intersection over Union)
  - ![image-20220311114402720](images/image-20220311114402720.png)

- 非极大值抑制

  - 可能有多个窗口同时检测到,选择重合部分最高的部分

    再选择可信度最高的一个

- Anchor Box
  - ![image-20220311115135953](images/image-20220311115135953.png)
  - ![image-20220311115158897](images/image-20220311115158897.png)

### 12.3 YOLO

- ![image-20220311115611061](images/image-20220311115611061.png)

- ![image-20220311115810312](images/image-20220311115810312.png)

- 候选区域
  - ![image-20220311134101478](images/image-20220311134101478.png)

## 13 人脸识别

### 13.1 One-Shot 学习

- 只能从一个样本处理,不能使用分类来处理了

- 使用相似性函数,判断两张照片的差异程度

- Siamese network

  ![image-20220311135310304](images/image-20220311135310304.png)

- 损失函数
  - ![image-20220311135642135](images/image-20220311135642135.png)



- ![image-20220311135852311](images/image-20220311135852311.png)

### 13.2 面部识别与二分类

- ![image-20220311140551493](images/image-20220311140551493.png)

### 13.3 神经风格转换

- 深度神经网络

  ![image-20220311141000963](images/image-20220311141000963.png)

![image-20220311141316597](images/image-20220311141316597.png)

- 损失函数

  ![image-20220311141453819](images/image-20220311141453819.png)

  ![image-20220311141826514](images/image-20220311141826514.png)![image-20220311142155247](images/image-20220311142155247.png)

- 用不同通道的相关性衡量风格

  ![image-20220311142809763](images/image-20220311142809763.png)



### 13.4 三维推广

- ![image-20220311143116657](images/image-20220311143116657.png)

## 14 循环神经网络

### 14.1 序列模型

- ![image-20220311143429150](images/image-20220311143429150.png)

- 符号标识

  ![image-20220311144003653](images/image-20220311144003653.png)

### 14.2 循环神经网络

- ![image-20220311144355222](images/image-20220311144355222.png)

![image-20220311144628268](images/image-20220311144628268.png)

- ![image-20220311144855072](images/image-20220311144855072.png)

- ![image-20220311145239185](images/image-20220311145239185.png)

![image-20220311145350512](images/image-20220311145350512.png)

### 14.3 语言模型

- ![image-20220311150020930](images/image-20220311150020930.png)

![image-20220311150354545](images/image-20220311150354545.png)

![image-20220311150613979](images/image-20220311150613979.png)

### 14.4 GRU

- 梯度爆炸可以使用梯度裁切得到处理![image-20220311150956251](images/image-20220311150956251.png)

- ![image-20220311151733194](images/image-20220311151733194.png)

![image-20220311151933186](images/image-20220311151933186.png)

### 14.5 LSTM

- ![image-20220311152249263](images/image-20220311152249263.png)

### 14.6 双向神经网络

- ![image-20220311152750815](images/image-20220311152750815.png)

- ![image-20220311153021132](images/image-20220311153021132.png)

## 15 NLP

### 15.1 词汇嵌入

- ![image-20220311155336932](images/image-20220311155336932.png)

- ![image-20220311161014327](images/image-20220311161014327.png)

- ![image-20220311161401010](images/image-20220311161401010.png)

- ![image-20220311161607040](images/image-20220311161607040.png)

- ![image-20220311161656587](images/image-20220311161656587.png)

- ![image-20220311161938706](images/image-20220311161938706.png)

- ![image-20220311162334790](images/image-20220311162334790.png)
- ![image-20220311162309260](images/image-20220311162309260.png)

..... TODO

























