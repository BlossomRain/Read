# 吴恩达机器学习笔记

- [来源](https://www.bilibili.com/video/BV164411b7dx?from=search&seid=11819125319968161620&spm_id_from=333.337.0.0)
- 笔记时间：2022.3.1

## 1 基础概念

- 应用场景：
  - 数据挖掘(网络点击、医学记录、生物、工程)
  - 无法手动编程(自然语言处理、计算机图像)
  - 个性推荐
  - AI

- 机器学习定义：

  - Arthur Samuel：使计算机不用编程而具有学习能力的研究领域

  - Tom Mitchell：计算机程序从经验E中学习解决某一任务T进行某一性能度量P,

    通过P测定在T上的表现因经验E而提高

    例子：垃圾邮件分类：E观测用户分类垃圾邮件,T程序分类垃圾邮件,P分类的准确率

### 1.1监督学习

- 训练数据带有“正确答案”
- 回归问题：预测连续值输出,比如某地区房价
- 分类问题：预测离散值输出,比如肿瘤大小与良性关系

### 1.2 无监督学习

- 训练数据没有“答案”
- 聚类：谷歌新闻,DNA工程,社交网络,市场分割,天文数据分析
- 鸡尾酒会问题：分离声音,通过两个麦克风记录声音

## 2 单变量线性回归

### 2.1 模型描述

- 算法会假设一个函数拟合数据进行预测,假设房价与面积的关系 h(x)=θ~0~+θ~1~x

- 如何确定那些参数(θ)更好？ 令Σ( y~i~ - y~pred~)^2^ 最小化,也就是误差最小

  假如取用绝对值无法求导,选择偶数幂保证大于等于0

  从另一个角度理解：衡量预测值和实际值之间的距离,而范数就是距离

  为了方便求导和归一化,乘以系数 1/2m,m是样本个数

### 2.2 损失函数

- 衡量预测值和实际值之间的差异,在线性回归里很适合使用,目标是最小化损失函数

- 均方误差MSE ![image-20220303160548929](images/image-20220303160548929.png)
- ![](images/image-20220304093653553.png)

### 2.3 梯度下降法最小化函数

- 初始化参数(通常为0),通过不断变化参数值,最小化损失函数。

  变化的方向是梯度反方向

- 可能会困在局部最优解

- θ~j~表示第j个参数,α表示学习率,偏导表示方向

  ![image-20220304100246609](images/image-20220304100246609.png)

- 左边:学习率太小,收敛太慢,

  右边:学习率太大,发散

  - 由公式可知,当偏导为0,参数不变 ==> 局部最优点 ==> 即使步长不变也可以在局部最优点
  - 可以每次都减少步长,但是没必要

  ![image-20220304101214475](images/image-20220304101214475.png)

### 2.4 线性回归与梯度下降

- 凸函数总是可以收敛到全局最优

- 注意点

  - 系数是1/2m,求导以后会消掉 2
  - h含有θ和x,所以展开后求导更方便理解,h(x)=θ~0~+θ~1~x,记得使用链式求导
  - 因为每次都需要对所有样本求和,所以称为批量梯度下降

  ![image-20220304102025067](images/image-20220304102025067.png)

## 3 线性代数

### 3.1 矩阵和向量

- 矩阵:按照长方形阵列排列的数集合,用大写字母表示
- 向量:某一列或某一行的矩阵元素,用小写字母表示
- A~ij~表示元素,y~i~表示向量分量

- 矩阵的线性运算:加法和数乘

- 矩阵乘法:不满足交换律,满足结合律,单位阵,

- 至此,可以将之前的假设函数表示成矩阵形式

  ![](images/0.png)

- 逆和转置



## 4 多元线性回归

### 4.1 多元特征

- 使用x~i~表示特征i,y表示实际值,x^i^表示样本i
- ![image-20220304111704652](images/image-20220304111704652.png)

### 4.2 多元梯度下降法

- ![](images/1.png)

### 4.3 梯度下降技巧

- 特征缩放:令数据都处于相似范围,用百分比
  - 处于[-1,1]范围
  - 处于[0,1]: (x-min)/(max-min)
  - 均值归一:  (x-μ)/(max-min)

- 学习率

  - 绘制 损失函数-迭代次数 曲线,可以直观看到收敛情况

    也可以通过阈值判断,比如小于10^-3^

  - 学习率过大可能导致发散,周期性,可以使用10^i^进行尝试

#### 4.4 多项式回归

- 类似幂级数,此时特征缩放就十分重要![image-20220304142158633](images/image-20220304142158633.png)

### 4.5 正规方程

- 区别于梯度下降使用迭代法,直接一步到位
- 令函数的一阶导数等于0,求得极值.因为是凸函数,所以是最值
- 正规方程不用特征缩放,假如特征是10^4^量级或以下可以考虑使用
- 矩阵不可逆如何处理?
  - 出现原因:数据重复,太多特征
  - 删除某些特征,正则化
- ![](images/3.png)



## 6 逻辑回归

### 6.1 分类

- 假如直接采用线性回归拟合,会出现一些问题

  - 分类输出是离散值,线性回归是连续值

- 假设函数还是特征的加权和,输出则是将连续值转为离散值

  ![image-20220304145634923](images/image-20220304145634923.png)

### 6.2 决策边界

- 由sigmoid函数图可知,当x>0,g(z)>0.5,假设判断 g(z)>=0.5 预测为1,那么 h(x)>=0就认为是1
- 决策边界不是训练集的属性,而是假设本身及其参数的属性
- ![image-20220304150458184](images/image-20220304150458184.png)![image-20220304151125654](images/image-20220304151125654.png)

### 6.3 损失函数

- 从线性回归的损失函数 MSE 进行尝试,假设每个样本的损失函数是 cost=1/2*( h(x) -y)^2^,

  通过对cost的绘图是凸函数,

  但是使用 sigmoid 的假设函数替代 h(x) 得到的将是非凸函数,如左图

  ![image-20220304152209426](images/image-20220304152209426.png)

  

- 使用的损失函数

  ![image-20220304154406641](images/image-20220304154406641.png)

### 6.4 分类与梯度下降

- 损失函数,可以用统计学的极大似然估计推出,(或者交叉熵?)

  ![image-20220304155255101](images/image-20220304155255101.png)

- 梯度下降--求导之后和线性回归是一致的

  ![image-20220304160042963](images/image-20220304160042963.png)

### 6.5 优化

- 共轭梯度

- BFGS

- L-BFGS

- 不用手动选择学习率,通常比梯度下降更快;

  更加复杂

### 6.6 多元分类

- 输出是  max h(x),概率最高的那一类

  ![image-20220304162410168](images/image-20220304162410168.png)

## 7 正则化

### 7.1 过拟合

- ![image-20220304165005259](images/image-20220304165005259.png)

- 正则化或者降维

### 7.2 线性回归

- 在损失函数的基础上假如惩罚项,调整不同参数的比重 

  ![image-20220304165858410](images/image-20220304165858410.png)



- 线性回归 梯度下降正则化

  ![image-20220304170445703](images/image-20220304170445703.png)

- 正规方程正则化
  
  - ![image-20220304170714561](images/image-20220304170714561.png)



### 7.3 逻辑回归

- ![image-20220307091824809](images/image-20220307091824809.png)



## 8 神经网络 模型

### 8.1 非线性假设

- 假设有100个特征,使用二次多项式拟合需要 N^2^/2 个参数.

  以图像处理为例,50x50的图片就有2500个特征

### 8.2 神经元与大脑

- 用一种通用算法模拟大脑处理不同的事件,生物神经重接实验

### 8.3 模型

- 生物神经元 -- 多输入单输出,![image-20220307100135431](images/image-20220307100135431.png)![image-20220307100655854](images/image-20220307100655854.png)

- 向量化,上图转为向量运算,从输入往输出逐个计算,称为前向传播![image-20220307101536409](images/image-20220307101536409.png)

### 8.4 例子

- 逻辑AND,没有隐藏层

  ![image-20220307104602546](images/image-20220307104602546.png)



- 逻辑 XNOR,具有一层隐藏层

  ![image-20220307105148311](images/image-20220307105148311.png)

### 8.5 多元分类

- 就像之前的多元分类,使用OvA

  ![image-20220307105944823](images/image-20220307105944823.png)



## 9 神经网络 学习

### 9.1 损失函数

- 和逻辑回归的形式类似,都不会计算偏置项

  ![image-20220307110427202](images/image-20220307110427202.png)

  ![image-20220307110518898](images/image-20220307110518898.png)

### 9.2 反向传播

- 前向传播案例![image-20220307111117780](images/image-20220307111117780.png)

- 反向传播,计算预测值和实际值的误差

  ![image-20220307111800816](images/image-20220307111800816.png)

### 9.3 梯度检测

- 梯度估计--利用临近点的切线

  用估计值和计算的梯度进行比较,如果很接近,那么梯度下降导数计算是正确的

  ε可以取值10^-4^等等

  ![image-20220307145841075](images/image-20220307145841075.png)

  ![image-20220307150045841](images/image-20220307150045841.png)

### 9.4 随机初始化

- 全是 0 : 导数,误差等权重都是一样的,更新后也会是相等的
- 随机初始化:权重可以在[-ε,ε]之间随机数,可以符合某些分布(高斯分布,均匀分布等)

## 10 机器学习建议

### 10.1 评估算法

- 划分数据集:训练集和测试集(8:2),计算模型误差
  
  - 可以使用MSE,CE,0/1分类错误测量等等
- 模型选择:利用交叉验证选择合适的参数模型

- 方差与偏差:

  - 偏差双高,方差一高一低 ------ 欠拟合与过拟合

    ![image-20220307160138146](images/image-20220307160138146.png)

- 正则化

  - 利用网格搜索自动选择合适λ

    ![image-20220307162436017](images/image-20220307162436017.png)![image-20220307162758856](images/image-20220307162758856.png)

- 学习曲线

  - 训练集和交叉验证 两者误差随着集合大小变化图

  - 高偏差情况下,更适合的特征   -- 欠拟合

    高方差情况下,考虑更多的数据   

    ![image-20220307163049156](images/image-20220307163049156.png)

## 11 机器学习系统设计

### 11.1确定优先级: 垃圾邮件分类案例

- 数据
  - 找到最常出现的一些单词,转成特征向量 
  - 搜集大量数据,收集日常邮件特征,错误拼写,标题和主体

- 误差分析:
  - 快速做出原型,简单粗暴即可,为了提取有用特征
  - 查看错误分类数据,尝试提取有用特征
- 不对称分类误差评估:
  - 混淆矩阵![image-20220307165753250](images/image-20220307165753250.png)
  - F1=2PR/(P+R),Precision,Recall

- 机器学习数据
  - 只要数据集够大就不怕过拟合

## 12 支持向量机

### 12.1 优化目标

- 逻辑回归的激活函数,希望如果真实y=0,那么令预测值等于0,也就是输入远小于0;

  ![image-20220307171127994](images/image-20220307171127994.png)

- 用品红色函数替代交叉熵,优化新的损失函数

  ![image-20220307171439871](images/image-20220307171439871.png)



### 12.2 大间隔

- ![image-20220307171737597](images/image-20220307171737597.png)

- 当损失函数方框部分为零,那么决定C大小的只有正则项

  只要把保证两个条件就可以令方框部分为0

  ![image-20220307172830430](images/image-20220307172830430.png)

- 数学原理

  - 内积是投影长度

  - SVM决策边界:最小化正则项

    p是映射到θ上的长度,p越大θ就可以越小
    
    ![image-20220307173145471](images/image-20220307173145471.png)![image-20220308090223984](images/image-20220308090223984.png)

### 12.3 核函数

- 衡量 x 和 l 的接近程度![image-20220308091009905](images/image-20220308091009905.png)

- 向量缩放优化,常用核 线性核,高斯核,多项式核,....

  ![image-20220308092133304](images/image-20220308092133304.png)



## 13 无监督学习

### 13.1 K-means算法

- 随机放置K个聚类中心,根据距离将样本进行分类

  移动聚类中心,新中心点为该类样本均值,重新标记分类

- 优化:最小化样本到中心的距离

  - ![image-20220308102351908](images/image-20220308102351908.png)

- 随机初始化:
  - 随机选择K个样本作为初始化中心
  - 随机初始化可能得到局部最优解
  - 多次运行,选取代价最小的,代价函数就是分类距离分类中心的距离

- 聚类数量K: 手动选择
  - 肘部法则,![image-20220308103334594](images/image-20220308103334594.png)
  - 根据需求确定K

## 14 降维

### 14.1 数据压缩

- ![image-20220308103829053](images/image-20220308103829053.png)

### 14.2 PCA

- 主成分分析问题规划
  - 找到一个超平面进行投影,让投影误差尽可能小
  - 特征需要均值归一化  (x-μ)/(len),μ是均值,len是需要放缩的长度(标准差,极差等)
- 算法
  - 计算协方差矩阵Σ
  - 进行SVD分解计算特征向量
  - 提取U矩阵前k个向量

- K值选择
  - ![image-20220308110124498](images/image-20220308110124498.png)
  - ![image-20220308110530865](images/image-20220308110530865.png)

- 压缩复原
  - ![image-20220308115128615](images/image-20220308115128615.png)

- 一些建议
  - 图像处理中应用PCA
  - 防止过拟合不适合用PCA
  - 能用原始数据就用原始数据,除非需要降维和压缩
  - 错误:PCA用于压缩文件

## 15 异常检测

### 15.1 问题动机

-  引擎问题检测    用户异常检测     数据中心检测![image-20220308134550263](images/image-20220308134550263.png)

### 15.2 高斯分布

- ![image-20220308135446909](images/image-20220308135446909.png)

- 参数估计(极大似然估计),这里m也可以使用m-1替代

  ![image-20220308135621326](images/image-20220308135621326.png)



### 15.3 算法 

- 密度估计,假设变量独立

![ ](images/image-20220308140153580.png)

- 划分数据集![image-20220308141239567](images/image-20220308141239567.png)
- 评估![image-20220308141502113](images/image-20220308141502113.png)

### 15.4 监督学习vs异常检测

- 区别在于数据量,异常检测很少负样本![image-20220308143814364](images/image-20220308143814364.png)

- 异常检测特征选择

  - 假如不是服从高斯分布也是可以工作的,可以使用转换变为高斯分布(比如取对数,指数等操作)
  - 组合成新的特征

- 多元高斯分布

  ![image-20220308145156854](images/image-20220308145156854.png)

- 多元高斯分布 -- 异常检测

  ![image-20220308145315776](images/image-20220308145315776.png)

- 分布选择

  ![image-20220308145731075](images/image-20220308145731075.png)

## 16 推荐系统

### 16.1 基于内容的推荐系统

- 已知电影类型,推算用户爱好,对于每个样本进行线性回归![image-20220308151718744](images/image-20220308151718744.png)



​	![image-20220308151812270](images/image-20220308151812270.png)

### 16.2 协同过滤

- 已知用户偏好推算电影类型

  ![image-20220308155207487](images/image-20220308155207487.png)

  

![image-20220308160334251](images/image-20220308160334251.png)

- 算法 

  ![image-20220308160938205](images/image-20220308160938205.png)



### 16.3 向量化

- ![image-20220308161643772](images/image-20220308161643772.png)

### 16.4 均值归一化

- 使用均值替代NaN

  ![image-20220308162245978](images/image-20220308162245978.png)

## 17 大规模机器学习

### 17.1 随机梯度下降

- 假如数据集1k能满足要求,就不用1亿数据训练

- 批量梯度下降要求全部训练数据求和,代价很大;

  用一个样本替代求和操作

  方向随机,但是会在最小值附近

  ![image-20220308163207629](images/image-20220308163207629.png)

  

- 小批量梯度下降 -- 使用若干个样本求导

- 随机梯度下降收敛

  - ![image-20220308164131397](images/image-20220308164131397.png)

  - 学习率可以随迭代下降,便于收敛.

    常数也会收敛,两者都可以

### 17.2 在线学习

- 每次只学习一个样本,不停学习   -- 适合数据流稳定的数据源

  ![image-20220308170513818](images/image-20220308170513818.png)

### 17.3 Map-Reduce 和 数据并行

- 很多学习算法都是基于求和,可以分解为map-reduce

  比如 线性回归,逻辑回归

  ![image-20220308171347596](images/image-20220308171347596.png)

- 也可以利用Map-Reduce实现多核并行运算

## 18 OCR

- 流水线: 图片 → 文字检测 → 字符分割 → 图片识别 ]

1. 滑动窗口: 文字检测(物体检测)
   - 从数据集收集大小相同的样本进行监督学习,比如 80*80
   - 然后窗口检测样本,移动下一步,再检测
   - 放大窗口,移动监测
   - 发现文字后进行形态学处理--膨胀腐蚀等操作,根据长宽比找出文字部分

2. 检测文字/字符分割部分
   - 流程与文字识别一致

3. 字符识别
   - 多分类问题

- 数据来源
  - 人工合成 -- 文字识别,字体+背景,字体计算机有很多
  - 数据扩充 -- 仿射变换,模糊,添加会出现的噪音







## 附录

### 单词表

- Supervised Learning 监督学习
- Regression 回归
- Breast  Cancer 乳腺癌
- tumor 肿瘤
- malignant 恶性,benign良性
- diabetes 糖尿病
- hypothesis   假设
- univariate 单变量
- convex凸函数
- batch批量

### 一些思路

#### **不能使用MSE当逻辑回归的损失函数**

![image-20220304153853904](images/image-20220304153853904.png)

#### **[反向传播算法](https://www.cnblogs.com/wlzy/p/7751297.html)**

- 损失函数是预测值和目标值的差的平方,利用梯度下降法,最小化损失函数

- 实际问题上只知道目标值和输出,如何优化中间的节点参数就成了问题

  根据箭头反方向求得偏导进行参数优化

- 这里的损失函数使用MSE,也可以用交叉熵

![image-20220307112809013](images/image-20220307112809013.png)

- [加上权重的案例](https://www.cnblogs.com/charlotte77/p/5629865.html)

  ![image-20220307113412131](images/image-20220307113412131.png)

  1. 计算前向传播,权重使用默认初始化,可以发现离目标值差距较大

     假设使用sigmoid激活函数,可以得到[0.75,0.77],目标[0.01,0.99]

  2. 反向传播

     1. 计算误差,使用MSE

     2. 计算不同的权重参数对误差的贡献度,链式法则

        ![image-20220307113910745](images/image-20220307113910745.png)

        ![img](images/853467-20160630153103187-515052589.png)

        ![image-20220307114331250](images/image-20220307114331250.png)

        ![image-20220307114657364](images/image-20220307114657364.png)





